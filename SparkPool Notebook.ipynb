{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-19T15:09:25.3705446Z",
              "execution_start_time": "2022-06-19T15:09:25.2151933Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-19T15:09:25.0121961Z",
              "session_id": 19,
              "session_start_time": null,
              "spark_pool": "vksynapsespark",
              "state": "finished",
              "statement_id": 2
            },
            "text/plain": [
              "StatementMeta(vksynapsespark, 19, 2, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import fsspec\n",
        "import pickle\n",
        "from azure.storage.blob import BlobClient\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-19T15:09:48.3806523Z",
              "execution_start_time": "2022-06-19T15:09:25.5057118Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-19T15:09:25.1280842Z",
              "session_id": 19,
              "session_start_time": null,
              "spark_pool": "vksynapsespark",
              "state": "finished",
              "statement_id": 3
            },
            "text/plain": [
              "StatementMeta(vksynapsespark, 19, 3, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = spark.read.load('abfss://tweetdata@dlsaviney.dfs.core.windows.net/tweets_temp.csv',format='csv',header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-19T15:10:05.2072141Z",
              "execution_start_time": "2022-06-19T15:09:48.4869924Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-19T15:09:25.463516Z",
              "session_id": 19,
              "session_start_time": null,
              "spark_pool": "vksynapsespark",
              "state": "finished",
              "statement_id": 4
            },
            "text/plain": [
              "StatementMeta(vksynapsespark, 19, 4, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/trusted-service-\n",
            "[nltk_data]     user/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /home/trusted-service-\n",
            "[nltk_data]     user/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /home/trusted-service-\n",
            "[nltk_data]     user/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
            "/home/trusted-service-user/cluster-env/clonedenv/lib/python3.8/site-packages/sklearn/base.py:313: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.21.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  warnings.warn(\n",
            "/home/trusted-service-user/cluster-env/clonedenv/lib/python3.8/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/home/trusted-service-user/cluster-env/clonedenv/lib/python3.8/site-packages/sklearn/base.py:313: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.21.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "fsspec_obj = fsspec.open('abfss://tweetdata@dlsaviney.dfs.core.windows.net/logistic_model.pkl')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "with fsspec_obj.open() as file:\n",
        "    bow_obj = pickle.load(file)\n",
        "    model = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-19T15:10:05.4537899Z",
              "execution_start_time": "2022-06-19T15:10:05.3113868Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-19T15:09:26.0081062Z",
              "session_id": 19,
              "session_start_time": null,
              "spark_pool": "vksynapsespark",
              "state": "finished",
              "statement_id": 5
            },
            "text/plain": [
              "StatementMeta(vksynapsespark, 19, 5, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def tokenization(data):\n",
        "    \"\"\"\n",
        "    :param data: It will receive the tweet and perform tokenization and remove the stopwords\n",
        "    :return: It will return the cleaned data\n",
        "    \"\"\"\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    stop_words.remove('no')\n",
        "    stop_words.remove('not')\n",
        "\n",
        "    tokenizer = nltk.tokenize.TweetTokenizer()\n",
        "\n",
        "    document = []\n",
        "    for text in data:\n",
        "        collection = []\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        for token in tokens:\n",
        "            if token not in stop_words:\n",
        "                if '#' in token:\n",
        "                    collection.append(token)\n",
        "                else:\n",
        "                    collection.append(re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\", \" \", token))\n",
        "        document.append(\" \".join(collection))\n",
        "    return document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-19T15:10:05.7171885Z",
              "execution_start_time": "2022-06-19T15:10:05.5668607Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-19T15:09:26.1801471Z",
              "session_id": 19,
              "session_start_time": null,
              "spark_pool": "vksynapsespark",
              "state": "finished",
              "statement_id": 6
            },
            "text/plain": [
              "StatementMeta(vksynapsespark, 19, 6, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def lemmatization(data):\n",
        "    \"\"\"\n",
        "    :param data: Receive the tokenized data\n",
        "    :return: Return the cleaned data\n",
        "    \"\"\"\n",
        "    lemma_function = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "    sentence = []\n",
        "    for text in data:\n",
        "        document = []\n",
        "        words = text.split(' ')\n",
        "        for word in words:\n",
        "            document.append(lemma_function.lemmatize(word))\n",
        "        sentence.append(\" \".join(document))\n",
        "    return sentence\n",
        "\n",
        "def get_tweet_sentiment(my_tweet) -> str:\n",
        "    \"\"\"\n",
        "        Here we'll perform predictions on the data given by the tweeter.\n",
        "    \"\"\"\n",
        "    tokenized_data = tokenization([my_tweet])\n",
        "    lemmatized_data = lemmatization(tokenized_data)\n",
        "    temp = bow_obj.transform(lemmatized_data)\n",
        "    pred = model.predict(temp)\n",
        "    if pred == 0:\n",
        "        return 'Positive'\n",
        "    else:\n",
        "        return 'Negative'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-19T15:20:07.4706838Z",
              "execution_start_time": "2022-06-19T15:20:03.5600714Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-19T15:20:03.3805229Z",
              "session_id": 19,
              "session_start_time": null,
              "spark_pool": "vksynapsespark",
              "state": "finished",
              "statement_id": 14
            },
            "text/plain": [
              "StatementMeta(vksynapsespark, 19, 14, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:289: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
            "  'JavaPackage' object is not callable\n",
            "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Converting Spark Dataframe to Pandas Dataframe\n",
        "tweet_df = df.toPandas()\n",
        "sentiment_results = []\n",
        "for tweet in tweet_df['Tweet']:\n",
        "    sentiment_results.append(get_tweet_sentiment(tweet))\n",
        "tweet_df ['Sentiments_Predictions'] = sentiment_results\n",
        "# Converting Pandas Dataframe into Spark Dataframe\n",
        "spark_df =spark.createDataFrame(tweet_df)\n",
        "# Create a temporary table view on the dataframe\n",
        "spark_df.createOrReplaceTempView(\"sentiment_data\")\n",
        "# Repartition used For Storing Spark Dataframe into single parquet file\n",
        "df2 = spark_df.repartition(1)\n",
        "# Loading Dataframe to parquet file in primary ADLS by giving directory and folder name\n",
        "df2.write.mode(\"append\").parquet('/data1/parquetfolder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "scala"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-19T15:10:33.1988733Z",
              "execution_start_time": "2022-06-19T15:10:33.0493156Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-19T15:09:26.4561128Z",
              "session_id": 19,
              "session_start_time": null,
              "spark_pool": "vksynapsespark",
              "state": "finished",
              "statement_id": 8
            },
            "text/plain": [
              "StatementMeta(vksynapsespark, 19, 8, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%spark\n",
        "// Making Scala Dataframe from Temporary table\n",
        "val scala_df = spark.sql(\"SELECT * FROM sentiment_data\")\n",
        "// For Loading Spark Dataframe into Synapse SQL Table by append mode\n",
        "scala_df.write.mode(\"append\").synapsesql(\"asa_twitter_db.dbo.tbl_tweet_sentiment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2022-06-19T15:22:04.3294338Z",
              "execution_start_time": "2022-06-19T15:22:02.5039644Z",
              "livy_statement_state": "available",
              "queued_time": "2022-06-19T15:22:02.4046291Z",
              "session_id": 19,
              "session_start_time": null,
              "spark_pool": "vksynapsespark",
              "state": "finished",
              "statement_id": 15
            },
            "text/plain": [
              "StatementMeta(vksynapsespark, 19, 15, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.widget-view+json": {
              "widget_id": "74122644-740f-4be9-8028-4ff806f29f0a",
              "widget_type": "Synapse.DataFrame"
            },
            "text/plain": [
              "SynapseWidget(Synapse.DataFrame, 74122644-740f-4be9-8028-4ff806f29f0a)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%pyspark\n",
        "df = spark.read.load('abfss://dlfsviney@dlsaviney.dfs.core.windows.net/data1/parquetfolder/part-00000-b87ae188-5159-413c-9a0e-a4daa795a1cb-c000.snappy.parquet', format='parquet')\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {
        "74122644-740f-4be9-8028-4ff806f29f0a": {
          "persist_state": {
            "view": {
              "chartOptions": {
                "aggregationType": "count",
                "categoryFieldKeys": [
                  "0"
                ],
                "chartType": "bar",
                "isStacked": false,
                "seriesFieldKeys": [
                  "0"
                ]
              },
              "tableOptions": {},
              "type": "details"
            }
          },
          "sync_state": {
            "isSummary": false,
            "language": "scala",
            "table": {
              "rows": [
                {
                  "0": "1538474868974313474",
                  "1": "VineyKhaneja",
                  "2": "GO TO SEVENTEEN",
                  "3": "2022-06-19 10:52:46+00:00",
                  "4": "Positive"
                },
                {
                  "0": "1538097094317072384",
                  "1": "VineyKhaneja",
                  "2": "Go to sixteen",
                  "3": "2022-06-18 09:51:38+00:00",
                  "4": "Positive"
                },
                {
                  "0": "1537434112847323143",
                  "1": "VineyKhaneja",
                  "2": "please incremental load pass hoja",
                  "3": "2022-06-16 13:57:11+00:00",
                  "4": "Positive"
                },
                {
                  "0": "1537414672663474176",
                  "1": "VineyKhaneja",
                  "2": "sqlpoolcheck",
                  "3": "2022-06-16 12:39:56+00:00",
                  "4": "Positive"
                },
                {
                  "0": "1534731762558898176",
                  "1": "VineyKhaneja",
                  "2": "verified !",
                  "3": "2022-06-09 02:59:00+00:00",
                  "4": "Positive"
                },
                {
                  "0": "1534489278389944322",
                  "1": "VineyKhaneja",
                  "2": "let us see",
                  "3": "2022-06-08 10:55:28+00:00",
                  "4": "Positive"
                },
                {
                  "0": "1534049121744482304",
                  "1": "VineyKhaneja",
                  "2": "hi",
                  "3": "2022-06-07 05:46:26+00:00",
                  "4": "Positive"
                },
                {
                  "0": "1534048912113213440",
                  "1": "VineyKhaneja",
                  "2": "RT @ShashiTharoor: Object lesson: Those who express bigotry at home should be aware of the consequences abroad. India has enjoyed a proud s…",
                  "3": "2022-06-07 05:45:36+00:00",
                  "4": "Positive"
                },
                {
                  "0": "1534048643077918721",
                  "1": "VineyKhaneja",
                  "2": "RT @narendramodi: India has won! भारत की विजय। अच्छे दिन आने वाले हैं।",
                  "3": "2022-06-07 05:44:32+00:00",
                  "4": "Positive"
                },
                {
                  "0": "1532641253212295169",
                  "1": "VineyKhaneja",
                  "2": "hey",
                  "3": "2022-06-03 08:32:04+00:00",
                  "4": "Positive"
                }
              ],
              "schema": [
                {
                  "key": "0",
                  "name": "tweet_id",
                  "type": "string"
                },
                {
                  "key": "1",
                  "name": "user_name",
                  "type": "string"
                },
                {
                  "key": "2",
                  "name": "Tweet",
                  "type": "string"
                },
                {
                  "key": "3",
                  "name": "ExactDate",
                  "type": "string"
                },
                {
                  "key": "4",
                  "name": "Sentiments_Predictions",
                  "type": "string"
                }
              ],
              "truncated": false
            }
          },
          "type": "Synapse.DataFrame"
        }
      },
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
